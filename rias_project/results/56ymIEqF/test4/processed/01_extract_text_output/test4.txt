MapFusion: A General Framework for 3D Object Detection
with HDMaps
Jin Fang1,2, Dingfu Zhou1,2, Xibin Song1,2, Liangjun Zhang1,2
Abstract— 3D object detection is a key perception component
in autonomous driving. Most recent approaches are based on
Lidar sensors only or fused with cameras. Maps (e.g., High
Deﬁnition Maps), a basic infrastructure for intelligent vehicles,
however, have not been well exploited for boosting object
detection tasks. In this paper, we propose a simple but effective
framework - MapFusion to integrate the map information into
modern 3D object detector pipelines. In particular, we design
a FeatureAgg module for HD Map feature extraction and
fusion, and a MapSeg module as an auxiliary segmentation
head for the detection backbone. Our proposed MapFusion is
detector independent and can be easily integrated into different
detectors. The experimental results of three different baselines
on large public autonomous driving dataset demonstrate the
superiority of the proposed framework. By fusing the map
information, we can achieve 1.27 to 2.79 points improvements
for mean Average Precision (mAP) on three strong 3d object
detection baselines.
I. INTRODUCTION
Autonomous driving (AD) has drawn signiﬁcant attention
over the past years. Despite recent progress, AD is still
considered as one of the most challenging tasks [1]. Devel-
oping autonomous driving systems needs to integrate many
advanced robotics techniques such as perception, planning,
and control. Perception module (e.g., object detection [2],
lane-marks segmentation, trafﬁc lights detection, etc.) is
particularly important in the AD system. Perception module
interprets the surrounding environment and provides inputs
for downstream planning and control modules. For object
detection tasks, cameras are the most commonly used sensors
and many advanced camera-based 2D detectors [3], [4] have
been developed. However, autonomous driving vehicles need
to recover 3D locations of surrounding objects accurately
and reliably, which are extremely hard for 2D image-based
detectors, due to the loss of distance information in the
perspective projection process from 3D space to 2D image.
Therefore, active 3D sensors such as LiDAR devices, are
prerequisites for most AD systems.
With the development of deep learning on the 3D point
clouds, many LiDAR-based 3D object detectors have been
proposed. Based on the different input representations, ap-
proaches can be generally categorized as point-based [6],
[7] and voxel-based methods [5], [8]. Compared with point-
based methods, the latter is much more efﬁcient and its
computation time is independent of the size of the point
1 Robotics and Autonomous Driving Laboratory, Baidu Research. 2
National Engineering Laboratory of Deep Learning Technology and Ap-
plication, China.
{fangjin, zhoudingfu, songxibin, liangjun
zhang}@baidu.com
(a)
(b)
Fig. 1: (a) shows the detection results from PointPillars [5], where
the false positives are marked by red cycles. (b) shows the detection
results from PointPillars with MapFusion, where the false positives
are removed beneﬁting from the MapFusion framework. The yellow
rectangles are the ground truth and the cyan rectangles are the
predict results.
cloud. The performance for existing LiDAR-based 3D object
detection approaches, however, can be further improved,
especially on reducing the false positives and false negatives
for detected objects. This problem comes from two main
reasons. First, without enough texture information, it is
difﬁcult to distinguish between foreground and background
objects. Second, the number of scanned points for small
objects or objects far away is very sparse, which results in
the failure of object detection and recognition. Fusing with
other sensors e.g., camera [9], radar [10] is a commonly used
strategy to handle this kind of problem. However, as image
quality is easily affected by the environment illumination and
weather situation, solely relying on fusion with images can’t
provide stable detection results. Fig. 1 (a) demonstrates the
3D detection results of PointPillars [5], and we can see that
false positives occurs due to the problems discussed above.
In this paper, we exploit boosting the performance of
perception tasks such as 3D object detection using High
Deﬁnition Map (HDMap). Different from regular GPS dig-
arXiv:2103.05929v1  [cs.CV]  10 Mar 2021

---PAGE BREAK---

MapSeg
Branch
3D Feature 
Extractor
2D Feature 
Extractor
濃濖濞濙濗濨澔
澸濙濨濙濗濨濝濣濢
2D
Backbone
Voxel 
Features
Map Feature
Fused 
Features
Input Points
Input Map
FeatureAgg
Standard 3D Object Detection
Map Feature Extraction
Fig. 2: The overview of our proposed MapFusion framework. The modules in the red dotted box are standard 3D object detection and the
elements in the blue dotted box are our map feature extraction block. The extracted map features are concatenated with the voxel features
for the following object detection branch. In addition, a sub-module called MapSeg branch is followed the voxel features for predicting
the road map from the input point cloud, which is supervised by the ground truth map.
ital maps, which are primarily meant for human naviga-
tion, HDMaps speciﬁcally have extremely high precision at
centimeter-level and include all topological and geometric
information of roads, such as lanes, trafﬁc signs, and side-
walk zones. Most existing 3D object detection approaches
only use the HDMap during the post-processing procedure
to remove false positive detection such as objects far from the
roads. Few approaches have been proposed to integrate the
HDMap information into the detection network to improve
the performance. In HDNET [11], a self-designed 3D object
detection pipeline has been designed for using HDMaps as
inputs, though their detection method is based on the bird-
eye-view representation by projecting the 3D point cloud into
2D.
In this work, we propose a general fusion framework
for integrating the HDMap information to boost the 3D
object detection task. We design a FeatureAgg module for
HDMap feature extraction and fusion, and a MapSeg module
as an auxiliary segmentation head for the detection back-
bone. More importantly, the designed fusion framework is
detector independent and can be employed for different 3D
object detection pipelines directly, such as SECOND [8],
PointPilars [5] and CenterPoint [12]. The effectiveness of
the proposed framework has been veriﬁed on the large-scale
public autonomous driving dataset-nuScenes [13]. As shown
in Fig. 1 (b), false positives can be partly alleviated, which
proves the effectiveness of our approach.
The contributions of the proposed work can be generally
summarized as:
1) We present MapFusion, a general data fusion frame-
work for 3D object detection in AD scenario, which is
more general, effective, and can be integrated into any
3D detection pipelines.
2) A light-weight network has been designed to extract
useful features from the HDMaps and the overhead for
inference time has only and false negatives increased
slightly compared with the baselines.
3) We evaluate the proposed framework on large-scale
public dataset nuScenes and the experimental results
demonstrate that the map information can signiﬁcantly
improve the performance of 3d object detection.
II. RELATED WORK
A. Lidar-based 3d Object Detection
With the development of range sensors and AD techniques,
3D object detection in driving scenarios draws more and
more attention. To solve the problem, one of the commonly
used strategy is projecting the 3D point cloud into 2D
(e.g., bird-eye-view [14] or front-view [15]) to obtain the
corresponding 2D detection result, then the ﬁnal result can
be obtained by re-projecting the 2D BBox into 3D.
Beneﬁting from the development of graphics process-
ing resources, volumetric convolutional approaches become
another representative direction for 3D object detection.
Voxelnet [16] is a pioneering method, which employs 3D
convolution to detect the 3D objects by converting the
LiDAR point cloud to voxels. Inspired by Voxelnet [16],
SECOND [8] and PointPillars [5] have been proposed,
which use different 3D voxel representations for 3D object
detection. By using structure information of 3D point cloud,
[17] proposes a novel framework, which can improve the
localization precision of single-stage detectors. Meanwhile,
CenterPoint [18] proposes to represent, detect, and track 3D
objects as points, which detects centers and other attributes
of the objects, then reﬁnes the estimated information using
additional point features on the objects.
Meanwhile, PointNet [19] proposes a novel technique
for point cloud feature extraction. Based on PointNet [19],
several state-of-the-art methods have been proposed for 3D
object detection [6], [20]–[22]. To avoid destroying the
hidden information about free space, [23] proposes to utilize
3D ray casting and batch-based gradient learning strategies
for 3D object detection.
Besides, [24] and [25] propose effective intersection over-
union (IoU) operations to generalize the losses in 3D object

---PAGE BREAK---

H ൈW ൈ3
H ൈW ൈ64 
H ൈW ൈ
ሼͳ͸ǡ ͳ͸ǡ ͵ʹǡ ͵ʹǡ ͸Ͷǡ͸Ͷ}
2D Feature Extractor
2D 
Conv
6 ൈ
Fig. 3: The architecture of the 2D Feature Extractor contains 6 similar layers. Each layer contains a 2D convolution operation, a batch
normalization operation and a ReLU function.
detection, which improve the accuracy of 3D object detec-
tion.
B. Fusion-based 3D object detection
Many approaches have been proposed for 3D object de-
tection to leverage fusion strategy for better performance.
MV3D [26] employs a compact multi-view representation
to encode the sparse 3D point cloud, which is composed of
3D object proposal generation and multi-view feature fusion.
Meanwhile, using LIDAR point cloud and RGB image as in-
put, AVOD [27] proposes neural network architecture for 3D
object detection, which consists of a region proposal network
(RPN) and a second stage detector network. Besides, to solve
the problem of localizing objects in point clouds of large-
scale scenes, Frustum pointnets [20] leverages both mature
2D object detectors and advanced 3D deep learning for 3D
object detection with RGB-D data as input. [7] proposes a
simple but practical detection framework to jointly predict
the 3D BBox and instance segmentation.
Meanwhile, 3D maps always contain geographic, geomet-
ric and semantic priors that can improve the performance
of many tasks. Using dense priors provided by large-scale
crowd-sourced maps, [28] proposes a novel framework to
build a holistic model for multi-tasks of 3D object detec-
tion, semantic segmentation and depth reconstruction. Early
attempt of fusing HDMaps, such as HDNet [11] shows that
strong priors provided by HDMaps can boost the perfor-
mance and robustness of modern 3D object detectors.
III. PROPOSED APPROACH
In this section, we present our MapFusion framework.
A. Overview
The overview of the proposed framework is illustrated in
Fig. 2 which can be roughly divided into two parts: the
standard 3D object detection block and the map feature
extraction block. The standard LiDAR-based 3d object de-
tection pipeline is described in the top red dotted box. The
input LiDAR point cloud is sent to a 3d feature extractor
such as 3D Sparse Convolution and outputs the features for
voxels. The map feature extraction block is denoted as the
blue dotted box, which takes the HDMap as the input. After
a 2D feature extractor, the map feature with the same size of
voxel feature is extracted. Then the 3D point cloud features
and the map information are aggregated with a concatenation
operation for each voxel. Then, a detection head including
the region proposals, box regression, and categories classiﬁ-
cation follows the fused features. In addition, an auxiliary
segmentation head, namely MapSeg, is added to further
improve the feature extraction capability. Our MapFusion is
an end-to-end framework that can be easily integrated into
any standard 3d object detection pipeline with only slight
modiﬁcations. Details of the MapFusion are introduced in
the following subsections. In addition, the detailed network
structure of 2D Feature Extractor and MapSeg are given in
the Fig.3 and Fig.4.
B. HDMap Representation
HDMaps contain rich information on road elements, such
as drivable areas, walking areas, and lanes. We use a raster
representation by rendering the semantic elements with ego
car at the center of the image. For the object detection tasks,
only three kinds of elements are chosen here which are
“drivable area”, “walkway”, and “carpark area” speciﬁcally.
Instead of using the three raster images directly for fusion,
we leverage a 2D Feature Extractor module to extract high-
level features from the three raster images. The structure of
2D Feature Extractor is illustrated in Fig. 3, which is a stack
of six similar layers including one 2D convolution with 3 × 3
kernels, batch normalization, and a ReLU activate function.
The ﬁlter number for the six layers are 16, 16, 32, 32, 32,
64, and 64 respectively. Speciﬁcally, we keep the image size
unchanged before and after the 2D Feature Extractor block.
C. FeatureAgg Module
The FeatureAgg module aims at fusing the extracted map
features and the voxel features. For simplicity, we keep the
voxel feature and the map feature with the same size and
concatenate the two tensors along the feature channel. Al-
though the operation is extremely simple, it gives satisfactory
fusion results. In addition, we ﬁnd that the performance
can be further improved if a 1×1 convolutional operation
is added before sending the concatenated features into the
next detection head. Further analysis can be found in the
ablation study part.

---PAGE BREAK---

2D
Conv
2D
Conv
Binary
Cross
Entropy
2D
Backbone
Voxel Feature
Ground Truth
Predict
Fig. 4: The architecture of MapSeg Module. Fill the voxel feature into the 2D backbone network and then follow two 2D Conv layers.
Finally, the binary cross-entropy loss is evaluated between the predicted segmentation map and the ground truth map.
D. MapSeg Module
MapSeg module is an auxiliary segmentation head that
takes the voxel feature as input and outputs the map seg-
mentation predictions. The prediction is supervised by the
ground-truth map image. The intention of this module is
to learn the road structure information from the input point
cloud directly. In fact, this kind of information is inherently
learn-able because the derivable areas mostly have distinctive
structures (e.g., ﬂat) than Non-driving zones.
The architecture of MapSeg is illustrated in Fig. 4. Due
to the possibility of overlap between different map elements
deﬁnition, a binary cross-entropy loss is employed here for
multi-label segmentation. The 2D Backbone used here is
similar to the detection head. Following the 2D backbone,
there are two additional 2D Conv layers which contain a
2D convolutional layer, batch normalization, and a ReLU
operation. MapSeg is only used during the training stage, it
will be removed during inference process.
E. Data Augmentation
Data augmentation is a commonly used strategy to in-
crease data diversity and is extremely important for deep
learning-based approaches for improving the model’s gen-
eralization ability. For LiDAR-based object detection, com-
monly used data augmentation strategies include random
rotation, ﬂipping, and scaling. To align the features extracted
from the point cloud and the HDMaps, for each sample, we
keep the augmentation parameters are the same for both the
two inputs.
IV. EXPERIMENTAL RESULTS
MapFusion is a general framework and can be easily
integrated into mainstream 3d object detection methods. In
this section, we ﬁrst introduce the baseline detectors that are
used to evaluate MapFusion. Then, more details of the used
dataset and the experiment setups are given. Quantitative and
qualitative results are demonstrated to prove the effectiveness
of the proposed MapFusion.
A. Baseline Detectors
Three state-of-the-art point cloud-based 3d object de-
tectors are compared here: 1) SECOND [8] utilizes the
sparse convolution to signiﬁcantly increase the speed of
both training and inference; 2) PointPillars [5] uses a pillar
(vertical columns) representation and regards the pillars as a
pseudo image. Then a standard 2D detection backbone can
then be employed; 3) CenterPoint [18] is a strong anchor-
free baseline, which ranks the top among all LiDAR-only
method in public nuScenes [13] and Waymo [29] dataset.
B. Implementation Details
To compare fairly, the same experiment settings are used
for the above baseline methods during training. The epoch
number is set as 20, the optimization algorithm is AdamW
[30] with a one-cycle learning rate policy, and the max
learning rates for PointPillars, SECOND, and CenterPoint
are 0.001, 0.001, and 0.003, while the weight decay is 0.01
and momentum is 0.9. The resolution of map rasterization
we use here is 128 × 128.
Data augmentation is applied to guarantee data diversity
and improve the robustness of the network. Our data augmen-
tation strategies include 1) random rotation from [−π
4 , π
4 ]
around the gravity axis, 2) random ﬂipping and 3) random
scaling from the uniform distribution 0.95 to 1.05. Follow
[13], 10 previous lidar sweeps are accumulated to leverage
the temporal information.
Note that for CenterPoint [18], ﬂip testing strategy and
deformable convolution are employed to improve the perfor-
mance as described by the original paper [12].
C. nuScenes Dataset
We evaluate our MapFusion framework on the challenging
nuScenes 3D object detection benchmark [13], since the
KITTI benchmark [31] does not provide HDMaps. nuScenes
is a large-scale dataset with a total of 1,000 scenes, where
700 scenes (28,130 samples) are used for training, 150 scenes
(6019 samples) are used for validation and 150 scenes (6008
samples) are used for testing. The samples (also named as
keyframes) in each video are annotated every 0.5s with a
full 360-degree view, and their point clouds are densiﬁed by
the 10 non-keyframe sweep frames, yielding around 300,000
point clouds with 5-dim representation (x, y, z, r, ∆t), where
r is the reﬂectance and ∆t describes the time lag to the
keyframe (ranging from 0s to 0.45s). Besides, nuScenes
requires detecting objects with full 3D boxes, attributes and
velocities for 10 classes, including cars, pedestrians, buses,
bicycles, etc.
For generating the raster images of HDMaps, NuScenes
dataset provides APIs to retrieve and query a certain record,
such as “drivable area”, “walkway”, and “carpark area”
speciﬁcally within the map layers.

---PAGE BREAK---

Methods
NDS(%)
AP (%)
mAP
Car
Pedestrian
Bus
Barrier
T.C.
Truck
Trailer
Moto.
Cons.
Bicycle
SECOND [8] (w/o MF)
60.80
49.62
80.72
76.78
65.49
58.82
57.33
48.30
33.85
38.72
19.07
17.14
SECOND [8] (w MF)
62.04
50.89
81.83
77.83
67.71
59.80
58.19
50.31
37.85
40.04
17.68
17.65
Improvement ↑
+1.24
+1.27
+1.11
+1.05
+2.22
+0.98
+0.86
+2.01
+4.00
+1.32
-1.39
+0.51
PointPillars [5] (w/o MF)
57.45
43.87
81.10
70.91
62.69
47.11
45.04
49.36
35.53
30.34
11.41
5.20
PointPillars [5] (w MF)
58.95
46.66
81.21
72.22
65.51
54.19
52.43
45.21
37.79
36.74
14.34
6.92
Improvement ↑
+1.50
+2.79
+0.11
+1.31
+2.82
+7.08
+7.39
-4.15
+2.26
+6.40
+2.93
+1.72
CenterPoint [18] (w/o MF)
67.13
59.43
85.97
85.46
68.50
68.20
69.43
58.25
38.81
59.57
19.21
40.94
CenterPoint [18] (w MF)
67.97
60.61
86.38
86.30
70.27
70.57
70.22
58.46
40.98
62.09
17.81
43.01
Improvement ↑
+0.84
+1.18
+0.41
+0.84
+1.77
+2.37
+0.79
+0.21
+2.17
+2.52
-1.40
+2.07
TABLE I: Evaluation results of MapFusion on nuScenes validation dataset. NDS and mAP mean nuScenes detection score and mean
Average Precision. MF is short for MapFusion. T.C., Moto. and Cons. are short for trafﬁc cone, motorcycle, and construction vehicle,
respectively. The improvements of each method are demonstrated in red, where “w” and “w/o” stand for “with” and “without” in short.
D. Evaluation Metrics
The ofﬁcial evaluation metrics are an average detection
accuracy among all classes. For 3D detection, the main
accuracy metric is mean Average Precision (mAP) [32]
and nuScenes detection score (NDS). The mAP uses a bir-
deye-view center distance < 0.5m, 1m, 2m, 4m instead
of standard 3D box IoU (Intersection-over-Union). NDS is
a weighted average of mAP and other attributes metrics,
including translation, scale, orientation, velocity, and other
box attributes [13].
E. Evaluation Results
We evaluate MapFusion on nuScenes dataset with the
three baseline detectors which are introduced before, includ-
ing SECOND, PointPillars and CenterPoint. Tab. I demon-
strates the quantitative results with and without the proposed
MapFusion (MF). As shown in Tab. I, the proposed Map
Fusion (MF) can effectively improve the performance of
the baselines in both nuScenes detection score (NDS) and
mean Average Precision (mAP), which sufﬁciently proves the
effectiveness of the proposed MapFusion. In speciﬁc, using
the proposed MapFusion (MF), for NDS, the improvements
of SECOND, PointPillars and CenterPoint are 1.24%, 1.50%
and 0.84%, while for mAP, the improvements of SECOND,
PointPillars and CenterPoint are 1.27%, 2.79% and 1.18%,
respectively. Meanwhile, as demonstrated in Tab. I, the
proposed MapFusion can improve the performances for most
classes, including small and large objects. Using MapFusion,
for the commonly used small objects, such as “Bicycle”, the
improvements achieve 0.51%, 1.72% and 2.07% for the three
baseline methods, and for large objects, such as “Barrier”,
the improvements achieve 0.98%, 7.08% and 2.37% for the
three baseline methods. Note that for “T.C.”, the average
precision (AP) of PointPillars achieves 54.19% with 7.39%
improvements.
V. ABLATION STUDIES
In this section, we explore the effectiveness and limitation
of MapFusion. Firstly we show the impact of MapSeg
branch and FeatureAgg module on the introduced 3d object
baseline detectors, then the strategy for 2d feature extraction
is further discussed. In the end, some visualization results
are presented. All the experiments are evaluated on nuScenes
validation dataset with the metrics deﬁned in Sec. IV-D.
A. Inﬂuence of FeatureAgg and MapSeg
As the most important components for MapFusion, an
ablation experiment is conducted to explore the impact of
FeatureAgg and MapSeg modules. The exact same baseline
detectors, dataset, metrics and implementation details are
used here following Sec. IV. The results are shown in Tab.
II, where we can clearly ﬁnd the effectiveness of both
FeatureAgg and MapSeg. With the help of MapSeg, the
NDS and mAP value improved 0.32% and 0.39% aver-
agely, respectively. In the meanwhile, FeatureAgg contributes
0.96% and 1.02% improvement averagely for NDS and mAP,
respectively. The results also reveal that FeatureAgg has
more impact on feature fusion than MapSeg, but the whole
framework beneﬁts the most by combining them.
Methods
NDS
mAP
SECOND baseline
60.80
49.62
SECOND w MapSeg
61.25
49.92
SECOND w FeatureAgg
61.46
50.19
SECOND w MapFusion
62.04
50.89
PointPillar baseline
57.45
43.87
PointPillar w MapSeg
57.82
44.61
PointPillar w FeatureAgg
58.69
45.64
PointPillar w MapFusion
58.95
46.66
CenterPoint baseline
67.13
59.43
CenterPoint w MapSeg
67.27
59.56
CenterPoint w FeatureAgg
67.91
60.14
CenterPoint w MapFusion
67.97
60.61
TABLE II: Evaluate the effectiveness of FeatureAgg and MapFu-
sion on public nuScenes validation dataset, where “w” stands for
“with”.
B. Inﬂuence of 2D Feature Extractor
In this section, we explore the effectiveness of 2D Feature
Extractor, two different settings are evaluated, including 1)
Simple Fusion, which directly use the original rendered
map image as feature; 2) Deep Fusion, which use the
feature generated by the proposed architecture (2D Feature
Extractor) in MapFusion.
The evaluation results can be ﬁnd in Tab III. Simple
Fusion with the map image directly also contribute to the
ﬁnal performance with 0.37% and 0.74% for NDS and
mAP, respectively. But Deep Fusion outperforms the Simple
Fusion with 1.13% and 2.05% for for NDS and mAP,

---PAGE BREAK---

respectively. The results show that Deep Fusion has better
performance, beneﬁts from the neighbor information aggre-
gation by convolutions operation. The structure of proposed
2D Feature Extractor is very simple but effective, with
a stronger network and pretrained on other dataset may
get better performance. Since related technologies on 2d
feature extraction have been widely researched, no further
exploration of relevant issues will be addressed here.
Methods
NDS
mAP
PointPillar baseline
57.45
43.87
PointPillar w Simple Fusion
57.82
44.61
PointPillar w Deep Fusion
58.95
46.66
TABLE III: Evaluate the effectiveness of 2D Feature Extractor on
public nuScenes validata dataset with PointPillars [5], where “w”
stands for “with”.
C. Different Feature Aggregation Methods
In this section, we explore the impact of different feature
aggregation methods, three strategies are used here, 1) Simple
Concat, which simply concatenate the voxel feature tensor
and map feature tensor along the channel dimension; 2) Deep
Concat v1, which use a 1 × 1 convolutional operation after
the feature concatenation, while the output channel number
of the convolution layer is the same with voxel feature tensor;
3) Deep Concat v2, which is similar with Deep Concat v1,
but the output channel number of the convolution layer keeps
the same with input, which is our proposed method.
The evaluation results are shown in IV, from where we
can ﬁnd that the 1 × 1 convolutional operation helps to
improve the performance, especially about 1% improvement
for mAP, due to its better feature aggregation ability between
two different features. Performance degrades with channel
number decrease after the 1×1 convolution in Deep Concat
v1 setting, which is normal after features compression.
Methods
NDS
mAP
PointPillar baseline
57.45
43.87
PointPillar w Simple Concat
58.65
45.67
PointPillar w Deep Concat v1
58.81
46.09
PointPillar w Deep Concat v2
58.95
46.66
TABLE IV: Evaluate the impact of different feature aggregation
method on public nuScenes validata dataset with PointPillars [5],
where “w” stand for “with”.
D. Visualization of MapSeg Results
Fig. 5 demonstrates the segmentation results of the pro-
posed MapSeg module, where (a) shows the original map
images (groundtruth), (b) shows the inference segmentation
results of the corresponding voxels and (c) shows the ﬁnal
segmentation results of MapSeg module. As shown in Fig. 5,
comparing with groundtruth, it is obvious to ﬁnd that the
proposed MapSeg module can effectively segment accurate
results, which can boost the performance of the proposed
approach.
Fig. 5: Visualization of segmentation results for the MapSeg
module. (a) shows the original map images (groundturth); (b) shows
the inference segmentation results of the corresponding voxels, and
(c) shows the ﬁnal segmentation results of MapSeg module.
(a)
(b)
Fig. 6: Qualitative detection results of CenterPoint [18] with the
proposed MapFusion, where (a) and (b) are displayed in front view
and bird-eye-view with map respectively.

---PAGE BREAK---

E.
Detection Results Visualization
Fig. 6 demonstrates the quantitative detection results of
CenterPoint [18] with the proposed MapFusion on the
nuScenes “val” dataset. (a) and (b) illustrates the results
of front view and bird-eye-view maps, respectively. We
can see that CenterPoint with the proposed MapFusion can
sufﬁciently detect all the 3D Bboxes of the objects, which
proves the effectiveness of the proposed approach.
VI. CONCLUSION AND FUTURE WORKS
In this work, we present a map data fusion framework
that can effectively explore the inherent relationship between
road structure and the perception task. By fusing the map in-
formation, the 3D object detection performance is improved
for all three different baseline methods. Qualitative analysis
shows that false-positive detection can be largely suppressed.
Currently, the proposed framework can fuse map information
with LiDAR point cloud, and in the future, we plan to extend
our framework to fuse the data from other sensors such as
radar, camera, etc with HDMaps. In addition, as HDmaps
might not be available everywhere, we plan to develop map
prediction methods and integrate widely accessible regular
maps with the MapFusion framework.
REFERENCES
[1] E. Yurtsever, J. Lambert, A. Carballo, and K. Takeda, “A survey of
autonomous driving: Common practices and emerging technologies,”
IEEE Access, vol. 8, pp. 58 443–58 469, 2020. 1
[2] E. Arnold, O. Y. Al-Jarrah, M. Dianati, S. Fallah, D. Oxtoby,
and A. Mouzakitis, “A survey on 3d object detection methods for
autonomous driving applications,” IEEE Transactions on Intelligent
Transportation Systems, vol. 20, no. 10, pp. 3782–3795, 2019. 1
[3] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-
time object detection with region proposal networks,” in Advances in
neural information processing systems, 2015, pp. 91–99. 1
[4] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu,
and A. C. Berg, “Ssd: Single shot multibox detector,” in European
conference on computer vision.
Springer, 2016, pp. 21–37. 1
[5] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom,
“Pointpillars: Fast encoders for object detection from point clouds,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2019, pp. 12 697–12 705. 1, 2, 4, 5, 6
[6] S. Shi, X. Wang, and H. Li, “Pointrcnn: 3d object proposal generation
and detection from point cloud,” in CVPR, 2019. 1, 2
[7] D. Zhou, J. Fang, X. Song, L. Liu, J. Yin, Y. Dai, H. Li, and R. Yang,
“Joint 3d instance segmentation and object detection for autonomous
driving,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2020, pp. 1839–1849. 1, 3
[8] Y. Yan, Y. Mao, and B. Li, “Second: Sparsely embedded convolutional
detection,” Sensors, vol. 18, no. 10, p. 3337, 2018. 1, 2, 4, 5
[9] M. Liang, B. Yang, Y. Chen, R. Hu, and R. Urtasun, “Multi-task multi-
sensor fusion for 3d object detection,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2019, pp.
7345–7353. 1
[10] Y. Wang, Z. Jiang, X. Gao, J.-N. Hwang, G. Xing, and H. Liu,
“Rodnet: Radar object detection using cross-modal supervision,” in
Proceedings of the IEEE/CVF Winter Conference on Applications of
Computer Vision, 2021, pp. 504–513. 1
[11] B. Yang, M. Liang, and R. Urtasun, “Hdnet: Exploiting hd maps for
3d object detection,” in Conference on Robot Learning. PMLR, 2018,
pp. 146–155. 2, 3
[12] T. Yin, X. Zhou, and P. Kr¨ahenb¨uhl, “Center-based 3d object detection
and tracking,” arXiv preprint arXiv:2006.11275, 2020. 2, 4
[13] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Kr-
ishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal
dataset for autonomous driving,” arXiv preprint arXiv:1903.11027,
2019. 2, 4, 5
[14] X. Chen, K. Kundu, Z. Zhang, H. Ma, S. Fidler, and R. Urtasun,
“Monocular 3d object detection for autonomous driving,” in Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2016, pp. 2147–2156. 2
[15] B. Wu, A. Wan, X. Yue, and K. Keutzer, “Squeezeseg: Convolutional
neural nets with recurrent crf for real-time road-object segmentation
from 3d lidar point cloud,” in 2018 IEEE International Conference on
Robotics and Automation (ICRA).
IEEE, 2018, pp. 1887–1893. 2
[16] Y. Zhou and O. Tuzel, “Voxelnet: End-to-end learning for point cloud
based 3d object detection,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2018, pp. 4490–4499. 2
[17] C. He, H. Zeng, J. Huang, X.-S. Hua, and L. Zhang, “Structure aware
single-stage 3d object detection from point cloud,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, 2020, pp. 11 873–11 882. 2
[18] T. Yin, X. Zhou, and P. Kr¨ahenb¨uhl, “Center-based 3d object detection
and tracking,” CVPR, 2021. 2, 4, 5, 6, 7
[19] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning
on point sets for 3d classiﬁcation and segmentation,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition,
2017, pp. 652–660. 2
[20] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum pointnets
for 3d object detection from rgb-d data,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2018, pp.
918–927. 2, 3
[21] S. Shi, Z. Wang, J. Shi, X. Wang, and H. Li, “From points to
parts: 3d object detection from point cloud with part-aware and part-
aggregation network,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2020. 2
[22] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, “Pv-
rcnn: Point-voxel feature set abstraction for 3d object detection,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2020, pp. 10 529–10 538. 2
[23] P. Hu, J. Ziglar, D. Held, and D. Ramanan, “What you see is what you
get: Exploiting visibility for 3d object detection,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2020, pp. 11 001–11 009. 2
[24] D. Zhou, J. Fang, X. Song, C. Guan, J. Yin, Y. Dai, and R. Yang, “Iou
loss for 2d/3d object detection,” in 2019 International Conference on
3D Vision (3DV).
IEEE, 2019, pp. 85–94. 2
[25] Z. Yang, Y. Sun, S. Liu, X. Shen, and J. Jia, “Std: Sparse-to-dense
3d object detector for point cloud,” in Proceedings of the IEEE
International Conference on Computer Vision, 2019, pp. 1951–1960.
2
[26] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object
detection network for autonomous driving,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2017,
pp. 1907–1915. 3
[27] J. Ku, M. Moziﬁan, J. Lee, A. Harakeh, and S. Waslander, “Joint
3d proposal generation and object detection from view aggregation,”
IROS, 2018. 3
[28] S. Wang, S. Fidler, and R. Urtasun, “Holistic 3d scene understand-
ing from a single geo-tagged image,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2015, pp.
3964–3972. 3
[29] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,
J. Guo, Y. Zhou, Y. Chai, B. Caine, et al., “Scalability in perception
for autonomous driving: Waymo open dataset,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2020, pp. 2446–2454. 4
[30] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”
in International Conference on Learning Representations, 2019.
[Online]. Available: https://openreview.net/forum?id=Bkg6RiCqY7 4
[31] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous
driving? the kitti vision benchmark suite,” in 2012 IEEE Conference
on Computer Vision and Pattern Recognition. IEEE, 2012, pp. 3354–
3361. 4
[32] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser-
man, “The pascal visual object classes (voc) challenge,” International
journal of computer vision, vol. 88, no. 2, pp. 303–338, 2010. 5
